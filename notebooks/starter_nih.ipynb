{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "derived-israeli",
   "metadata": {},
   "source": [
    "# Starter notebook for NIH Chest Xray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-carter",
   "metadata": {},
   "source": [
    "## Copy data from GCS (if not so yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-specific setting\n",
    "PROJECT = 'mcsds-dlh-free'  # CHANGE: billing project name (since the dataset is user-to-pay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from GCS. Takes a few minutes.\n",
    "# https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest#gcp_data_access\n",
    "\n",
    "DATA_FOLDER = '../data/'\n",
    "!gsutil -u {PROJECT} -m -q cp -r gs://gcs-public-data--healthcare-nih-chest-xray/png/*.png {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download addition labels\n",
    "# https://pubs.rsna.org/doi/10.1148/radiol.2019191293\n",
    "\n",
    "!gsutil -u {PROJECT} -m -q cp -r gs://gcs-public-data--healthcare-nih-chest-xray-labels/* {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-primary",
   "metadata": {},
   "source": [
    "# Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# check if CUDA is available (GPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the dataset\n",
    "# load data label\n",
    "df_train_labels = pd.read_csv(os.path.join(DATA_FOLDER, 'train.csv'))\n",
    "df_valid_labels = pd.read_csv(os.path.join(DATA_FOLDER, 'valid.csv'))\n",
    "\n",
    "print(f\"Train samples: {df_train_labels.shape[0]}\")\n",
    "print(f\"Valid samples: {df_valid_labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get doc on features\n",
    "df_train_labels.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-terrorism",
   "metadata": {},
   "source": [
    "For labels: **blank** for unmentioned, **0** for negative, **-1** for uncertain, and **1** for positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "i=1\n",
    "\n",
    "print(df_train_labels['Path'][i])\n",
    "img=mpimg.imread(df_train_labels['Path'][i])\n",
    "#print(img.shape)  # 2D. Size varies.\n",
    "#print(img.max(), img.min())  # grayscale 0-255\n",
    "print(df_train_labels['Age'][i])\n",
    "print(df_train_labels['Sex'][i])\n",
    "print(df_train_labels['Frontal/Lateral'][i])\n",
    "print(df_train_labels['AP/PA'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_labels.loc[0:13,'Sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-lawyer",
   "metadata": {},
   "source": [
    "Note that the size of images varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-camcorder",
   "metadata": {},
   "source": [
    "Take the label *Pleural Effusion* as pivot, let's build a classifier for it.\n",
    "\n",
    "Settings:\n",
    "1. Consider only Frontal view images.\n",
    "2. U-MultiClass for Pleural Effusion - treat the uncertainty label (-1) as its own class.\n",
    "3. Null(NA) values are treated as *negative (0)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select training images\n",
    "def select_images(df):\n",
    "    df = df[df['Frontal/Lateral']=='Frontal']\n",
    "    df = df[['Path', 'Pleural Effusion']].fillna(0)  # Note that '0' is negative, '-1' is uncertain. We assume NA => Unmentioned => Negative => 0.\n",
    "    return df.reset_index()\n",
    "\n",
    "df_frontal = select_images(df_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frontal.describe(include='all')\n",
    "df_frontal['Pleural Effusion'].hist()\n",
    "\n",
    "df_frontal.groupby(['Pleural Effusion']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and validation data\n",
    "df_train = select_images(df_train_labels)\n",
    "df_valid = select_images(df_valid_labels)\n",
    "\n",
    "print(f\"# training images: {df_train.shape[0]}\")\n",
    "print(f\"# validation images: {df_valid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-staff",
   "metadata": {},
   "source": [
    "**Warning: The validation images serve as test set. Do NOT use them for model tuning.**\n",
    "Use leave-out set/CV on training images for tuning instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-arcade",
   "metadata": {},
   "source": [
    "Now we have the images and labels. We can train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "target_dict = {\n",
    "    -1: 2,  # uncertain\n",
    "    0: 0,  # negative\n",
    "    1: 1  # positive\n",
    "}\n",
    "\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, label, transform=None):\n",
    "        \"\"\"\n",
    "        label: column name of the label of interest, e.g. 'Pleural Effusion'.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.dataframe.loc[idx, 'Path'])\n",
    "        image = Image.open(img_name)\n",
    "        target = self.dataframe.loc[idx, self.label]\n",
    "        target = target_dict[target]  # map labels to 0,...,num_classes\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, target)\n",
    "\n",
    "# For testing/debug\n",
    "ds= CheXpertDataset(df_train, ROOT_PATH, 'Pleural Effusion')\n",
    "img, label = ds[191027-1]\n",
    "print(label)\n",
    "print(img.size)\n",
    "imgplot = plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader: train data loader (type: torch.utils.data.DataLoader)\n",
    "# val_loader: val data loader (type: torch.utils.data.DataLoader)\n",
    "def load_data(dataframe, root_dir, label, transform=None, batch_size=32, shuffle=True, num_workers=4):\n",
    "    '''\n",
    "    Data Loader with batch loading and transform.\n",
    "    '''\n",
    "    image_data = CheXpertDataset(dataframe, root_dir, label, transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(image_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    return loader\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),  # data augmentation\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-snapshot",
   "metadata": {},
   "source": [
    "Define our CNN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def conv_output_volume(W, F, S, P):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the input volume size $W$, the kernel/filter size $F$, \n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border, \n",
    "    calculate the output volume size.\n",
    "    \"\"\"\n",
    "    return int((W - F + 2*P) / S) + 1\n",
    "\n",
    "def maxpool_output_volume(W, F, S):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the input volume size $W$, the kernel/filter size $F$, \n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border, \n",
    "    calculate the output volume size.\n",
    "    \"\"\"\n",
    "    return int(np.ceil((W - F + 1) / S))\n",
    "\n",
    "conv_layer1_size = conv_output_volume(W=224, F=5, S=1, P=0)\n",
    "maxpool_layer1_size = maxpool_output_volume(W=conv_layer1_size, F=2, S=2)\n",
    "\n",
    "conv_layer2_size = conv_output_volume(W=maxpool_layer1_size, F=5, S=1, P=0)\n",
    "maxpool_layer2_size = maxpool_output_volume(W=conv_layer2_size, F=2, S=2)\n",
    "\n",
    "print(conv_layer1_size, maxpool_layer1_size, conv_layer2_size, maxpool_layer2_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, just use a simple one from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html, plus dropout\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5)  # stride=1, padding=0\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(3, 8, 5)\n",
    "        self.fc1 = nn.Linear(8 * 53 * 53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)  # 3 classes\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # size: (batch_size*)3channels*110*110\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # size: (batch_size*)8channels*53*53\n",
    "        x = x.view(-1, 8 * 53 * 53)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "import time\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 1024\n",
    "\n",
    "train_data_loader = load_data(df_train, ROOT_PATH, 'Pleural Effusion', transform=data_transforms['train'], shuffle=True, batch_size=batch_size)\n",
    "val_data_loader = load_data(df_valid, ROOT_PATH, 'Pleural Effusion', transform=data_transforms['val'], shuffle=False, batch_size=batch_size)\n",
    "\n",
    "print(f\"Training start. Mode: {device}\")\n",
    "start_time = time.time()\n",
    "\n",
    "net.train()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_data_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        # cast label data type to int\n",
    "        labels = labels.type(torch.LongTensor).to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            print(f\"Average time per batch: {(time.time()-start_time)/(i+1)} secs\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f'Finished Training. Total time: {time.time()-start_time} secs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save state\n",
    "MODEL_PATH = './simple_net.pth'\n",
    "torch.save(net.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation set\n",
    "\n",
    "# Load the saved model if necessary\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "        Y_pred: prediction of model on the dataloder.\n",
    "            Should be an 2D numpy float array where the second dimension has length 2.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    Y_prob = []\n",
    "    Y_pred = []\n",
    "    Y_test = []\n",
    "    for data, target in dataloader:\n",
    "        outputs = model(data.to(device))\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        Y_prob.append(probs)\n",
    "        Y_pred.append(predicted)\n",
    "        Y_test.append(target)\n",
    "        \n",
    "    Y_prob = np.concatenate(Y_prob, axis=0)\n",
    "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return Y_prob, Y_pred, Y_test\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "# lower batch size if out of memory\n",
    "train_data_loader = load_data(df_train, ROOT_PATH, 'Pleural Effusion', transform=data_transforms['train'], shuffle=True, batch_size=64)\n",
    "val_data_loader = load_data(df_valid, ROOT_PATH, 'Pleural Effusion', transform=data_transforms['val'], shuffle=False, batch_size=64)\n",
    "\n",
    "y_prob, y_pred, y_true = eval_model(net, train_data_loader)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "roc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "print((\"Train Accuracy: \" + str(acc)))\n",
    "print((\"Train ROC: \" + str(roc)))\n",
    "\n",
    "y_prob, y_pred, y_true = eval_model(net, val_data_loader)\n",
    "#print(y_prob)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "#roc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "\n",
    "print((\"Validation Accuracy: \" + str(acc)))\n",
    "#print((\"Validation ROC: \" + str(roc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear GPU memory\n",
    "!nvidia-smi  # show the PID\n",
    "#!kill 8210\n",
    "#!nvidia-smi  # check"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
