{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anonymous-dividend",
   "metadata": {},
   "source": [
    "# Starter notebook for NIH Chest Xray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-spanish",
   "metadata": {},
   "source": [
    "## Copy data from GCS (if not so yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-specific setting\n",
    "PROJECT = 'mcsds-dlh'  # CHANGE: billing project name (since the dataset is user-to-pay)\n",
    "DATA_FOLDER = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from GCS. Takes a few minutes.\n",
    "# https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest#gcp_data_access\n",
    "\n",
    "#!gsutil -u {PROJECT} -m -q cp -r gs://gcs-public-data--healthcare-nih-chest-xray/png/*.png {DATA_FOLDER}\n",
    "\n",
    "# Download addition labels\n",
    "# https://pubs.rsna.org/doi/10.1148/radiol.2019191293\n",
    "\n",
    "#!gsutil -u {PROJECT} -m -q cp -r gs://gcs-public-data--healthcare-nih-chest-xray-labels/* {DATA_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-desktop",
   "metadata": {},
   "source": [
    "# Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "# check if CUDA is available (GPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the dataset\n",
    "# load train test split\n",
    "with open('train_val_list.txt') as f: \n",
    "    train_val_list = [x.strip() for x in f.readlines()]\n",
    "with open('test_list.txt') as f:\n",
    "    test_list = [x.strip() for x in f.readlines()]\n",
    "\n",
    "# load labels\n",
    "df_labels = pd.read_csv('Data_Entry_2017_v2020.csv')\n",
    "print(f\"Number of images: {len(df_labels)}\")\n",
    "# split the finding (disease) labels, to a list\n",
    "df_labels['targets'] = df_labels['Finding Labels'].str.split(\"|\", expand = False)\n",
    "# look at available labels\n",
    "labels = set([item for sublist in df_labels['targets'].tolist() for item in sublist])\n",
    "\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "# one-hot encode labels to columns\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "df_labels = df_labels.join(\n",
    "            pd.DataFrame.sparse.from_spmatrix(\n",
    "                mlb.fit_transform(df_labels.pop('targets')),\n",
    "                index=df_labels.index,\n",
    "                columns=mlb.classes_))\n",
    "df_labels[list(labels)]=df_labels[list(labels)].sparse.to_dense()  # for easy .describe()\n",
    "\n",
    "# show converted data\n",
    "df_labels[['Finding Labels', *list(labels)]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train_val and test sets\n",
    "df_train_val = df_labels[df_labels['Image Index'].isin(train_val_list)]\n",
    "df_test = df_labels[df_labels['Image Index'].isin(test_list)]\n",
    "\n",
    "print(f\"Number of train/val images: {len(df_train_val)}\")\n",
    "print(f\"Number of test images: {len(df_test)}\")\n",
    "\n",
    "assert (len(df_train_val) + len(df_test)) == len(df_labels), \"Total number of images does not equal to sum of train/val and test!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read images\n",
    "i=54756\n",
    "\n",
    "print(df_labels['Image Index'][i])\n",
    "path_to_img = os.path.join('../data', df_labels['Image Index'][i])\n",
    "\n",
    "img=mpimg.imread(path_to_img)\n",
    "print(f\"Image size: {img.shape}\")  # 2D\n",
    "print(img.max(), img.min())  # grayscale [0.0, 1.0]\n",
    "print(df_labels['Patient Age'][i])\n",
    "print(df_labels['Patient Gender'][i])\n",
    "print(df_labels['View Position'][i])  # only AP/PA, no lateral\n",
    "# Plot an image\n",
    "imgplot = plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-muscle",
   "metadata": {},
   "source": [
    "Take the label *Atelectasis* as pivot, let's build a classifier for it.\n",
    "\n",
    "Settings:\n",
    "1. Consider only PA view images.\n",
    "2. Binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'Atelectasis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "df_labels.describe(include='all')\n",
    "df_labels[disease].hist()\n",
    "print(f\"Fraction of positive class: {len(df_labels[df_labels[disease]==1])/len(df_labels):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df_train_val, test_size=0.2,random_state=seed)  # 20% val set, about same size as test\n",
    "\n",
    "assert len(df_train) + len(df_val) == len(df_train_val)\n",
    "\n",
    "# Prepare train/val and test data\n",
    "def select_images(df):\n",
    "    df = df[df['View Position']=='PA'].reset_index()\n",
    "    return df\n",
    "\n",
    "df_train_pa = select_images(df_train)\n",
    "df_val_pa = select_images(df_val)\n",
    "df_test_pa = select_images(df_test)\n",
    "\n",
    "print(f\"# train images: {df_train_pa.shape[0]}\")\n",
    "print(f\"# val images: {df_val_pa.shape[0]}\")\n",
    "print(f\"# test images: {df_test_pa.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pa.loc[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-rolling",
   "metadata": {},
   "source": [
    "**Warning: The validation images serve as test set. Do NOT use them for model tuning.**\n",
    "Use leave-out set/CV on training images for tuning instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-footwear",
   "metadata": {},
   "source": [
    "Now we have the images and labels. We can train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "\n",
    "class NihDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, label, transform=None):\n",
    "        \"\"\"\n",
    "        label: column name of the label of interest, e.g. 'Pleural Effusion'.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.dataframe.loc[idx, 'Image Index'])\n",
    "        image = Image.open(img_name).convert('L')  # via .getband(), some images are know to have 4 channels. Here we convert them to 1-channel grayscale.\n",
    "        target = self.dataframe.loc[idx, disease]\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders to return batch of images\n",
    "def load_data(dataframe, root_dir, label, transform=None, batch_size=32, shuffle=True, num_workers=4):\n",
    "    '''\n",
    "    Data Loader with batch loading and transform.\n",
    "    '''\n",
    "    image_data = NihDataset(dataframe, root_dir, label, transform=transform)\n",
    "    pin = device=='cpu'\n",
    "    loader = torch.utils.data.DataLoader(image_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin)\n",
    "    return loader\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((112,112)),\n",
    "        transforms.RandomHorizontalFlip(),  # data augmentation\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((128,128)),\n",
    "        transforms.CenterCrop(112),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-washer",
   "metadata": {},
   "source": [
    "Define our CNN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def conv_output_volume(W, F, S, P):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the input volume size $W$, the kernel/filter size $F$, \n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border, \n",
    "    calculate the output volume size.\n",
    "    \"\"\"\n",
    "    return int((W - F + 2*P) / S) + 1\n",
    "\n",
    "def maxpool_output_volume(W, F, S):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given the input volume size $W$, the kernel/filter size $F$, \n",
    "    the stride $S$, and the amount of zero padding $P$ used on the border, \n",
    "    calculate the output volume size.\n",
    "    \"\"\"\n",
    "    return int(np.ceil((W - F + 1) / S))\n",
    "\n",
    "conv_layer1_size = conv_output_volume(W=112, F=5, S=1, P=0)\n",
    "maxpool_layer1_size = maxpool_output_volume(W=conv_layer1_size, F=2, S=2)\n",
    "\n",
    "conv_layer2_size = conv_output_volume(W=maxpool_layer1_size, F=5, S=1, P=0)\n",
    "maxpool_layer2_size = maxpool_output_volume(W=conv_layer2_size, F=2, S=2)\n",
    "\n",
    "print(conv_layer1_size, maxpool_layer1_size, conv_layer2_size, maxpool_layer2_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, just use a simple one from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html, plus dropout\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5)  # stride=1, padding=0\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(3, 8, 5)\n",
    "        self.fc1 = nn.Linear(8 * 25 * 25, 60)\n",
    "        self.fc2 = nn.Linear(60, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)  # 2 classes\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # size: (batch_size*)3channels*110*110\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # size: (batch_size*)8channels*53*53\n",
    "        x = x.view(-1, 8 * 25 * 25)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = torch.sigmoid(self.fc3(x))  # change to softmax if multiclass\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()  # change to CrossEntropyLoss if  multiclass\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, num_epochs=10, threshold=0.5):\n",
    "    start_time = time.time()\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.type(torch.FloatTensor).to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs.squeeze(-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics every 10 batches\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (i+1)))\n",
    "                print(f'Time elapsed: {(time.time()-start_time)/60.0:.1f} minutes.')\n",
    "        train_losses.append(running_loss/len(train_data_loader))  # keep trace of train loss in each epoch\n",
    "        \n",
    "        # validate every epoch\n",
    "        print(\"Validating...\")\n",
    "        val_loss, _, _, _ = eval_model(model, valid_loader, threshold=threshold)\n",
    "        val_losses.append(val_loss/len(val_data_loader))  # keep trace of validation loss in each epoch\n",
    "        print(\"Epoch: {}/{}.. \".format(epoch+1, num_epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Validation Loss: {:.3f}.. \".format(val_losses[-1]))\n",
    "        \n",
    "        # save the model every epoch\n",
    "        MODEL_PATH = f'../models/simple_net_{time.time()}_epoch{num_epochs}.pth'\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print(\"Model saved!\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def eval_model(model, loader, threshold=0.5):\n",
    "    from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "    # validate every epoch\n",
    "    loss = 0.0\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # empty tensors to hold results\n",
    "        Y_prob, Y_true, Y_pred = [], [], []\n",
    "        for inputs, labels in loader:\n",
    "            probs = model(inputs.to(device)).squeeze(-1)  # prediction probability\n",
    "            labels = labels.type(torch.FloatTensor).to(device)  # true labels\n",
    "            predicted = probs > threshold  # predicted labels               \n",
    "            # stack results\n",
    "            Y_prob.append(probs)\n",
    "            Y_true.append(labels)\n",
    "            Y_pred.append(predicted)\n",
    "            \n",
    "            loss += criterion(probs, labels)\n",
    "\n",
    "    # convert to numpy\n",
    "    Y_prob = torch.cat(Y_prob).detach().cpu().numpy()\n",
    "    Y_pred = torch.cat(Y_pred).detach().cpu().numpy()\n",
    "    Y_true = torch.cat(Y_true).detach().cpu().numpy()\n",
    "\n",
    "    # TODO: use desired metrics here\n",
    "    print(f\"ROC: {roc_auc_score(Y_true, Y_prob):.3f}\")\n",
    "    fpr, tpr, _ = roc_curve(Y_true, Y_prob)\n",
    "    auc = roc_auc_score(Y_true, Y_prob)\n",
    "    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    \n",
    "    print(classification_report(Y_true, Y_pred))\n",
    "    cm = confusion_matrix(Y_true, Y_pred)  # confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "     \n",
    "    return loss, Y_prob, Y_pred, Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 2048\n",
    "\n",
    "train_data_loader = load_data(df_train_pa, DATA_FOLDER, disease, transform=data_transforms['train'], shuffle=True, batch_size=batch_size)\n",
    "val_data_loader = load_data(df_val_pa, DATA_FOLDER, disease, transform=data_transforms['test'], shuffle=False, batch_size=32)\n",
    "\n",
    "print(f\"Training start. Mode: {device}\")\n",
    "start_time = time.time()\n",
    "model = train_model(model, train_data_loader, val_data_loader, num_epochs=num_epochs, threshold=0.10)\n",
    "print(f'Finished Training. Total time: {time.time()-start_time} secs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_data_loader = load_data(df_test_pa, DATA_FOLDER, disease, transform=data_transforms['test'], shuffle=False, batch_size=32)\n",
    "test_loss = eval_model(model, test_data_loader, threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-residence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
