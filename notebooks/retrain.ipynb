{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inside-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdrgv/.local/lib/python3.8/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
      "  from .collection import imread_collection_wrapper\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU count:1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import sklearn\n",
    "import sklearn.metrics as sklm\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# image imports\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import time\n",
    "from shutil import copyfile\n",
    "from shutil import rmtree\n",
    "\n",
    "# data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(\"Available GPU count:\" + str(gpu_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuck-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXRDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            path_to_images,\n",
    "            fold,\n",
    "            transform=None,\n",
    "            sample=0,\n",
    "            finding=\"any\",\n",
    "            starter_images=False):\n",
    "\n",
    "        self.transform = transform\n",
    "        self.path_to_images = path_to_images\n",
    "        self.df = pd.read_csv(\"nih_labels.csv\")\n",
    "        self.df = self.df[self.df['fold'] == fold]\n",
    "\n",
    "            \n",
    "        # can limit to sample, useful for testing\n",
    "        # if fold == \"train\" or fold ==\"val\": sample=500\n",
    "        if(sample > 0 and sample < len(self.df)):\n",
    "            self.df = self.df.sample(sample)\n",
    "\n",
    "        if not finding == \"any\":  # can filter for positive findings of the kind described; useful for evaluation\n",
    "            if finding in self.df.columns:\n",
    "                if len(self.df[self.df[finding] == 1]) > 0:\n",
    "                    self.df = self.df[self.df[finding] == 1]\n",
    "                else:\n",
    "                    print(\"No positive cases exist for \"+LABEL+\", returning all unfiltered cases\")\n",
    "            else:\n",
    "                print(\"cannot filter on finding \" + finding +\n",
    "                      \" as not in data - please check spelling\")\n",
    "\n",
    "        self.df = self.df.set_index(\"Image Index\")\n",
    "        self.PRED_LABEL = [\n",
    "            'Atelectasis',\n",
    "            'Cardiomegaly',\n",
    "            'Effusion',\n",
    "            'Infiltration',\n",
    "            'Mass',\n",
    "            'Nodule',\n",
    "            'Pneumonia',\n",
    "            'Pneumothorax',\n",
    "            'Consolidation',\n",
    "            'Edema',\n",
    "            'Emphysema',\n",
    "            'Fibrosis',\n",
    "            'Pleural_Thickening',\n",
    "            'Hernia']\n",
    "        RESULT_PATH = \"results/\"\n",
    "\n",
    "        self.df[\"Patient Gender\"]=self.df[\"Patient Gender\"].replace(to_replace =\"F\",value =0)\n",
    "        self.df[\"Patient Gender\"]=self.df[\"Patient Gender\"].replace(to_replace =\"M\",value =1)\n",
    "        self.df.loc[self.df[\"Patient Age\"].str.contains(pat = 'M'),\"Patient Age\"]='0'\n",
    "        self.df.loc[self.df[\"Patient Age\"].str.contains(pat = 'D'),\"Patient Age\"]='0'\n",
    "        self.df[\"Patient Age\"]=self.df[\"Patient Age\"].str.strip('Y')\n",
    "        self.df[\"Patient Age\"] = pd.to_numeric(self.df[\"Patient Age\"])\n",
    "        self.df[\"Patient Age\"]=(self.df[\"Patient Age\"]-self.df[\"Patient Age\"].min())/(self.df[\"Patient Age\"].max()-self.df[\"Patient Age\"].min())\n",
    "        self.df[\"View Position\"]=self.df[\"View Position\"].replace(to_replace =\"PA\",value =0)\n",
    "        self.df[\"View Position\"]=self.df[\"View Position\"].replace(to_replace =\"AP\",value =1)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.path_to_images,\n",
    "                self.df.index[idx]))\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "        label = np.zeros(len(self.PRED_LABEL), dtype=int)\n",
    "        for i in range(0, len(self.PRED_LABEL)):\n",
    "             # can leave zero if zero, else make one\n",
    "            if(self.df[self.PRED_LABEL[i].strip()].iloc[idx].astype('int') > 0):\n",
    "                label[i] = self.df[self.PRED_LABEL[i].strip()\n",
    "                                   ].iloc[idx].astype('int')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tabular = self.df.loc[self.df.index == self.df.index[idx],[\"Patient Age\", \"Patient Gender\",\"View Position\"]].values[0].tolist()\n",
    "        tabular = torch.FloatTensor(tabular)\n",
    "\n",
    "        return (image,tabular ,label,self.df.index[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "absolute-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMAGES = \"data/\"\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "iraqi-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_datasets = {}\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.Scale(224),\n",
    "#         # because scale doesn't always give 224 x 224, this ensures 224 x\n",
    "#         # 224\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Scale(224),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)\n",
    "#     ]),\n",
    "# }\n",
    "# transformed_datasets['val'] = CXRDataset(\n",
    "#     path_to_images=PATH_TO_IMAGES,\n",
    "#     fold='val',\n",
    "#     transform=data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "clear-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in transformed_datasets['val']:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "starting-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineModel(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(CombineModel, self).__init__()\n",
    "        self.resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.resnet18.fc.in_features\n",
    "        self.resnet18.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "        )\n",
    "        \n",
    "        self.ln4 = nn.Linear(3, 10)\n",
    "        self.ln5 = nn.Linear(10, 10)\n",
    "        self.ln6 = nn.Linear(10, 14)\n",
    "        self.ln7 = nn.Linear(28, 14)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid =nn.Sigmoid()\n",
    "\n",
    "    def forward(self, img,tab):\n",
    "        \n",
    "        img = self.resnet18(img)\n",
    "        \n",
    "        \n",
    "        tab = self.ln4(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln5(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln6(tab)\n",
    "        tab = self.relu(tab)\n",
    "#         print(img.shape)\n",
    "#         print(tab.shape)\n",
    "        x = torch.cat((img, tab), dim=1)\n",
    "        x = self.ln7(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modified-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(PATH_TO_IMAGES, LR, WEIGHT_DECAY):\n",
    "    \"\"\"\n",
    "    Train torchvision model to NIH data given high level hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        PATH_TO_IMAGES: path to NIH images\n",
    "        LR: learning rate\n",
    "        WEIGHT_DECAY: weight decay parameter for SGD\n",
    "\n",
    "    Returns:\n",
    "        preds: torchvision model predictions on test fold with ground truth for comparison\n",
    "        aucs: AUCs for each train,test tuple\n",
    "\n",
    "    \"\"\"\n",
    "    NUM_EPOCHS = 100\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "    try:\n",
    "        rmtree('results/')\n",
    "    except BaseException:\n",
    "        pass  # directory doesn't yet exist, no need to clear it\n",
    "    os.makedirs(\"results/\")\n",
    "\n",
    "    # use imagenet mean,std for normalization\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    N_LABELS = 14  # we are predicting 14 labels\n",
    "\n",
    "    # load labels\n",
    "    df = pd.read_csv(\"nih_labels.csv\", index_col=0)\n",
    "\n",
    "    # define torchvision transforms\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Scale(224),\n",
    "            # because scale doesn't always give 224 x 224, this ensures 224 x\n",
    "            # 224\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Scale(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # create train/val dataloaders\n",
    "    transformed_datasets = {}\n",
    "    transformed_datasets['train'] = CXRDataset(\n",
    "        path_to_images=PATH_TO_IMAGES,\n",
    "        fold='train',\n",
    "        transform=data_transforms['train'])\n",
    "    transformed_datasets['val'] = CXRDataset(\n",
    "        path_to_images=PATH_TO_IMAGES,\n",
    "        fold='val',\n",
    "        transform=data_transforms['val'])\n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "        transformed_datasets['train'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    dataloaders['val'] = torch.utils.data.DataLoader(\n",
    "        transformed_datasets['val'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "\n",
    "    model = CombineModel(N_LABELS).cuda()\n",
    "\n",
    "\n",
    "    # define criterion, optimizer for training\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(\n",
    "        filter(\n",
    "            lambda p: p.requires_grad,\n",
    "            model.parameters()),\n",
    "        lr=LR,\n",
    "        momentum=0.9,\n",
    "        weight_decay=WEIGHT_DECAY)\n",
    "    dataset_sizes = {x: len(transformed_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "    # train model\n",
    "    model, best_epoch = train_model(model, criterion, optimizer, LR, num_epochs=NUM_EPOCHS,\n",
    "                                    dataloaders=dataloaders, dataset_sizes=dataset_sizes, weight_decay=WEIGHT_DECAY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "regulated-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, best_loss, epoch, LR):\n",
    "    \"\"\"\n",
    "    Saves checkpoint of torchvision model during training.\n",
    "\n",
    "    Args:\n",
    "        model: torchvision model to be saved\n",
    "        best_loss: best val loss achieved so far in training\n",
    "        epoch: current epoch of training\n",
    "        LR: current learning rate in training\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    print('saving')\n",
    "    state = {\n",
    "        'model': model,\n",
    "        'best_loss': best_loss,\n",
    "        'epoch': epoch,\n",
    "        'rng_state': torch.get_rng_state(),\n",
    "        'LR': LR\n",
    "    }\n",
    "\n",
    "    torch.save(state, 'results/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "black-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        LR,\n",
    "        num_epochs,\n",
    "        dataloaders,\n",
    "        dataset_sizes,\n",
    "        weight_decay):\n",
    "    \"\"\"\n",
    "    Fine tunes torchvision model to NIH CXR data.\n",
    "\n",
    "    Args:\n",
    "        model: torchvision model to be finetuned (densenet-121 in this case)\n",
    "        criterion: loss criterion (binary cross entropy loss, BCELoss)\n",
    "        optimizer: optimizer to use in training (SGD)\n",
    "        LR: learning rate\n",
    "        num_epochs: continue training up to this many epochs\n",
    "        dataloaders: pytorch train and val dataloaders\n",
    "        dataset_sizes: length of train and val datasets\n",
    "        weight_decay: weight decay parameter we use in SGD with momentum\n",
    "    Returns:\n",
    "        model: trained torchvision model\n",
    "        best_epoch: epoch on which best model val loss was obtained\n",
    "\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    start_epoch = 1\n",
    "    best_loss = 999999\n",
    "    best_epoch = -1\n",
    "    last_train_loss = -1\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # set model to train or eval mode based on whether we are in train or\n",
    "        # val; necessary to get correct predictions given batchnorm\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            i = 0\n",
    "            total_done = 0\n",
    "            # iterate over all data in train/val dataloader:\n",
    "            for data in dataloaders[phase]:\n",
    "                i += 1\n",
    "                inputs,tab,labels, _ = data\n",
    "                batch_size = inputs.shape[0]\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                tab = Variable(tab.cuda())\n",
    "                labels = Variable(labels.cuda()).float()\n",
    "                outputs = model(inputs,tab)\n",
    "\n",
    "                # calculate gradient and update parameters in train phase\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                last_train_loss = epoch_loss\n",
    "\n",
    "            print(phase + ' epoch {}:loss {:.4f} with data size {}'.format(\n",
    "                epoch, epoch_loss, dataset_sizes[phase]))\n",
    "\n",
    "            # decay learning rate if no val loss improvement in this epoch\n",
    "\n",
    "            if phase == 'val' and epoch_loss > best_loss:\n",
    "                print(\"decay loss from \" + str(LR) + \" to \" +\n",
    "                      str(LR / 10) + \" as not seeing improvement in val loss\")\n",
    "                LR = LR / 10\n",
    "                # create new optimizer with lower learning rate\n",
    "                optimizer = optim.SGD(\n",
    "                    filter(\n",
    "                        lambda p: p.requires_grad,\n",
    "                        model.parameters()),\n",
    "                    lr=LR,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=weight_decay)\n",
    "                print(\"created new optimizer with LR \" + str(LR))\n",
    "\n",
    "            # checkpoint model if has best val loss yet\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "                checkpoint(model, best_loss, epoch, LR)\n",
    "\n",
    "            # log training and validation loss over each epoch\n",
    "            if phase == 'val':\n",
    "                with open(\"results/log_train\", 'a') as logfile:\n",
    "                    logwriter = csv.writer(logfile, delimiter=',')\n",
    "                    if(epoch == 1):\n",
    "                        logwriter.writerow([\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "                    logwriter.writerow([epoch, last_train_loss, epoch_loss])\n",
    "\n",
    "        total_done += batch_size\n",
    "        if(total_done % (100 * batch_size) == 0):\n",
    "            print(\"completed \" + str(total_done) + \" so far in epoch\")\n",
    "\n",
    "        # break if no val loss improvement in 3 epochs\n",
    "        if ((epoch - best_epoch) >= 3):\n",
    "            print(\"no improvement in 3 epochs, break\")\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights to return\n",
    "    checkpoint_best = torch.load('results/checkpoint')\n",
    "    model = checkpoint_best['model']\n",
    "\n",
    "    return model, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "orange-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train epoch 1:loss 0.1660 with data size 78468\n",
      "val epoch 1:loss 0.1564 with data size 11219\n",
      "saving\n",
      "Epoch 2/100\n",
      "----------\n",
      "train epoch 2:loss 0.1543 with data size 78468\n",
      "val epoch 2:loss 0.1529 with data size 11219\n",
      "saving\n",
      "Epoch 3/100\n",
      "----------\n",
      "train epoch 3:loss 0.1500 with data size 78468\n",
      "val epoch 3:loss 0.1505 with data size 11219\n",
      "saving\n",
      "Epoch 4/100\n",
      "----------\n",
      "train epoch 4:loss 0.1469 with data size 78468\n",
      "val epoch 4:loss 0.1501 with data size 11219\n",
      "saving\n",
      "Epoch 5/100\n",
      "----------\n",
      "train epoch 5:loss 0.1441 with data size 78468\n",
      "val epoch 5:loss 0.1500 with data size 11219\n",
      "saving\n",
      "Epoch 6/100\n",
      "----------\n",
      "train epoch 6:loss 0.1416 with data size 78468\n",
      "val epoch 6:loss 0.1514 with data size 11219\n",
      "decay loss from 0.01 to 0.001 as not seeing improvement in val loss\n",
      "created new optimizer with LR 0.001\n",
      "Epoch 7/100\n",
      "----------\n",
      "train epoch 7:loss 0.1314 with data size 78468\n",
      "val epoch 7:loss 0.1495 with data size 11219\n",
      "saving\n",
      "Epoch 8/100\n",
      "----------\n",
      "train epoch 8:loss 0.1276 with data size 78468\n",
      "val epoch 8:loss 0.1509 with data size 11219\n",
      "decay loss from 0.001 to 0.0001 as not seeing improvement in val loss\n",
      "created new optimizer with LR 0.0001\n",
      "Epoch 9/100\n",
      "----------\n",
      "train epoch 9:loss 0.1243 with data size 78468\n",
      "val epoch 9:loss 0.1515 with data size 11219\n",
      "decay loss from 0.0001 to 1e-05 as not seeing improvement in val loss\n",
      "created new optimizer with LR 1e-05\n",
      "Epoch 10/100\n",
      "----------\n",
      "train epoch 10:loss 0.1234 with data size 78468\n",
      "val epoch 10:loss 0.1509 with data size 11219\n",
      "decay loss from 1e-05 to 1.0000000000000002e-06 as not seeing improvement in val loss\n",
      "created new optimizer with LR 1.0000000000000002e-06\n",
      "no improvement in 3 epochs, break\n",
      "Training complete in 38m 1s\n"
     ]
    }
   ],
   "source": [
    "train_cnn(PATH_TO_IMAGES, LEARNING_RATE, WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fewer-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "directed-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_best = torch.load('results/checkpoint')\n",
    "model = checkpoint_best['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "optimum-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "model.train(False)\n",
    "\n",
    "# create dataloader\n",
    "dataset = CXRDataset(\n",
    "    path_to_images=PATH_TO_IMAGES,\n",
    "    fold=\"test\",\n",
    "    transform=data_transforms['val'])\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "size = len(dataset)\n",
    "\n",
    "# create empty dfs\n",
    "prob_df = pd.DataFrame(columns=[\"Image Index\"])\n",
    "true_df = pd.DataFrame(columns=[\"Image Index\"])\n",
    "pred_df = pd.DataFrame(columns=[\"Image Index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cardiovascular-badge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "160\n",
      "320\n",
      "480\n",
      "640\n",
      "800\n",
      "960\n",
      "1120\n",
      "1280\n",
      "1440\n",
      "1600\n",
      "1760\n",
      "1920\n",
      "2080\n",
      "2240\n",
      "2400\n",
      "2560\n",
      "2720\n",
      "2880\n",
      "3040\n",
      "3200\n",
      "3360\n",
      "3520\n",
      "3680\n",
      "3840\n",
      "4000\n",
      "4160\n",
      "4320\n",
      "4480\n",
      "4640\n",
      "4800\n",
      "4960\n",
      "5120\n",
      "5280\n",
      "5440\n",
      "5600\n",
      "5760\n",
      "5920\n",
      "6080\n",
      "6240\n",
      "6400\n",
      "6560\n",
      "6720\n",
      "6880\n",
      "7040\n",
      "7200\n",
      "7360\n",
      "7520\n",
      "7680\n",
      "7840\n",
      "8000\n",
      "8160\n",
      "8320\n",
      "8480\n",
      "8640\n",
      "8800\n",
      "8960\n",
      "9120\n",
      "9280\n",
      "9440\n",
      "9600\n",
      "9760\n",
      "9920\n",
      "10080\n",
      "10240\n",
      "10400\n",
      "10560\n",
      "10720\n",
      "10880\n",
      "11040\n",
      "11200\n",
      "11360\n",
      "11520\n",
      "11680\n",
      "11840\n",
      "12000\n",
      "12160\n",
      "12320\n",
      "12480\n",
      "12640\n",
      "12800\n",
      "12960\n",
      "13120\n",
      "13280\n",
      "13440\n",
      "13600\n",
      "13760\n",
      "13920\n",
      "14080\n",
      "14240\n",
      "14400\n",
      "14560\n",
      "14720\n",
      "14880\n",
      "15040\n",
      "15200\n",
      "15360\n",
      "15520\n",
      "15680\n",
      "15840\n",
      "16000\n",
      "16160\n",
      "16320\n",
      "16480\n",
      "16640\n",
      "16800\n",
      "16960\n",
      "17120\n",
      "17280\n",
      "17440\n",
      "17600\n",
      "17760\n",
      "17920\n",
      "18080\n",
      "18240\n",
      "18400\n",
      "18560\n",
      "18720\n",
      "18880\n",
      "19040\n",
      "19200\n",
      "19360\n",
      "19520\n",
      "19680\n",
      "19840\n",
      "20000\n",
      "20160\n",
      "20320\n",
      "20480\n",
      "20640\n",
      "20800\n",
      "20960\n",
      "21120\n",
      "21280\n",
      "21440\n",
      "21600\n",
      "21760\n",
      "21920\n",
      "22080\n",
      "22240\n",
      "22400\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "\n",
    "    inputs,tab ,labels, _ = data\n",
    "    inputs,tab ,labels = Variable(inputs.cuda()),Variable(tab.cuda()), Variable(labels.cuda())\n",
    "\n",
    "    true_labels = labels.cpu().data.numpy()\n",
    "    batch_size = true_labels.shape\n",
    "\n",
    "    outputs = model(inputs,tab)\n",
    "    probs = outputs.cpu().data.numpy()\n",
    "    predicted = probs > 0.1\n",
    "    \n",
    "    # get predictions and true values for each item in batch\n",
    "    for j in range(0, batch_size[0]):\n",
    "        thisrow = {}\n",
    "        truerow = {}\n",
    "        predrow = {}\n",
    "        thisrow[\"Image Index\"] = dataset.df.index[BATCH_SIZE * i + j]\n",
    "        truerow[\"Image Index\"] = dataset.df.index[BATCH_SIZE * i + j]\n",
    "        predrow[\"Image Index\"] = dataset.df.index[BATCH_SIZE * i + j]\n",
    "\n",
    "        # iterate over each entry in prediction vector; each corresponds to\n",
    "        # individual label\n",
    "        for k in range(len(dataset.PRED_LABEL)):\n",
    "            thisrow[\"prob_\" + dataset.PRED_LABEL[k]] = probs[j, k]\n",
    "            truerow[dataset.PRED_LABEL[k]] = true_labels[j, k]\n",
    "            predrow[dataset.PRED_LABEL[k]] = predicted[j, k]\n",
    "        prob_df = prob_df.append(thisrow, ignore_index=True)\n",
    "        true_df = true_df.append(truerow, ignore_index=True)\n",
    "        pred_df = pred_df.append(predrow, ignore_index=True)\n",
    "\n",
    "    if(i % 10 == 0):\n",
    "        print(str(i * BATCH_SIZE))\n",
    "\n",
    "auc_df = pd.DataFrame(columns=[\"label\", \"auc\"])\n",
    "acc_df = pd.DataFrame(columns=[\"label\", \"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "emerging-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in true_df:\n",
    "\n",
    "    if column not in ['Atelectasis','Cardiomegaly','Effusion',\n",
    "        'Infiltration','Mass','Nodule','Pneumonia',\n",
    "        'Pneumothorax','Consolidation','Edema',\n",
    "        'Emphysema','Fibrosis','Pleural_Thickening','Hernia']:\n",
    "        continue\n",
    "    actual = true_df[column]\n",
    "    prob = prob_df[\"prob_\" + column]\n",
    "    thisrow = {}\n",
    "    thisrow['label'] = column\n",
    "    thisrow['auc'] = np.nan\n",
    "    thisrow['auc'] = sklm.roc_auc_score(\n",
    "        actual.to_numpy().astype(int), prob.to_numpy())\n",
    "    auc_df = auc_df.append(thisrow, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "olympic-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atelectasis</td>\n",
       "      <td>0.801996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0.897987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consolidation</td>\n",
       "      <td>0.796294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Edema</td>\n",
       "      <td>0.884148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Effusion</td>\n",
       "      <td>0.879006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emphysema</td>\n",
       "      <td>0.902379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fibrosis</td>\n",
       "      <td>0.805017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hernia</td>\n",
       "      <td>0.822729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Infiltration</td>\n",
       "      <td>0.703561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mass</td>\n",
       "      <td>0.810268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nodule</td>\n",
       "      <td>0.744628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pleural_Thickening</td>\n",
       "      <td>0.768077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>0.742446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>0.863802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label       auc\n",
       "0          Atelectasis  0.801996\n",
       "1         Cardiomegaly  0.897987\n",
       "2        Consolidation  0.796294\n",
       "3                Edema  0.884148\n",
       "4             Effusion  0.879006\n",
       "5            Emphysema  0.902379\n",
       "6             Fibrosis  0.805017\n",
       "7               Hernia  0.822729\n",
       "8         Infiltration  0.703561\n",
       "9                 Mass  0.810268\n",
       "10              Nodule  0.744628\n",
       "11  Pleural_Thickening  0.768077\n",
       "12           Pneumonia  0.742446\n",
       "13        Pneumothorax  0.863802"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "finite-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in true_df:\n",
    "\n",
    "    if column not in ['Atelectasis','Cardiomegaly','Effusion',\n",
    "        'Infiltration','Mass','Nodule','Pneumonia',\n",
    "        'Pneumothorax','Consolidation','Edema',\n",
    "        'Emphysema','Fibrosis','Pleural_Thickening','Hernia']:\n",
    "        continue\n",
    "    actual = true_df[column]\n",
    "    pred = pred_df[column]\n",
    "    thisrow = {}\n",
    "    thisrow['label'] = column\n",
    "    thisrow['acc'] = np.nan\n",
    "    thisrow['acc'] = sklm.accuracy_score(\n",
    "        actual.to_numpy().astype(int), pred.to_numpy())\n",
    "    acc_df = acc_df.append(thisrow, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "solved-excitement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atelectasis</td>\n",
       "      <td>0.722507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0.927874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consolidation</td>\n",
       "      <td>0.857754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Edema</td>\n",
       "      <td>0.938885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Effusion</td>\n",
       "      <td>0.759194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emphysema</td>\n",
       "      <td>0.964739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fibrosis</td>\n",
       "      <td>0.970668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hernia</td>\n",
       "      <td>0.998128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Infiltration</td>\n",
       "      <td>0.455401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mass</td>\n",
       "      <td>0.886774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nodule</td>\n",
       "      <td>0.866937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pleural_Thickening</td>\n",
       "      <td>0.917755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Pneumonia</td>\n",
       "      <td>0.988945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pneumothorax</td>\n",
       "      <td>0.910534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label       acc\n",
       "0          Atelectasis  0.722507\n",
       "1         Cardiomegaly  0.927874\n",
       "2        Consolidation  0.857754\n",
       "3                Edema  0.938885\n",
       "4             Effusion  0.759194\n",
       "5            Emphysema  0.964739\n",
       "6             Fibrosis  0.970668\n",
       "7               Hernia  0.998128\n",
       "8         Infiltration  0.455401\n",
       "9                 Mass  0.886774\n",
       "10              Nodule  0.866937\n",
       "11  Pleural_Thickening  0.917755\n",
       "12           Pneumonia  0.988945\n",
       "13        Pneumothorax  0.910534"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
